{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3edc26a8",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984a7819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75b1d59",
   "metadata": {},
   "source": [
    "## Load Model and Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fb2684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model and scaler\n",
    "model = joblib.load('fraud_detection_model.pkl')\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "print(\"Model and scaler loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd11121",
   "metadata": {},
   "source": [
    "## Load Model Comparison Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ae63a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model comparison data\n",
    "try:\n",
    "    comparison_df = pd.read_csv('model_comparison.csv')\n",
    "    print(\"Model comparison data loaded successfully!\")\n",
    "    print(comparison_df)\n",
    "except FileNotFoundError:\n",
    "    print(\"Model comparison data not available. Please run the training script first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1bee15",
   "metadata": {},
   "source": [
    "## Transaction Input and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b8be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example transaction details\n",
    "# You can modify these values to test different transactions\n",
    "transaction_data = {\n",
    "    'Time': 1000.0,\n",
    "    'V1': 0.0,\n",
    "    'V2': 0.0,\n",
    "    'V3': 0.0,\n",
    "    'V4': 0.0,\n",
    "    'V5': 0.0,\n",
    "    'V6': 0.0,\n",
    "    'V7': 0.0,\n",
    "    'V8': 0.0,\n",
    "    'V9': 0.0,\n",
    "    'V10': 0.0,\n",
    "    'V11': 0.0,\n",
    "    'V12': 0.0,\n",
    "    'V13': 0.0,\n",
    "    'V14': 0.0,\n",
    "    'V15': 0.0,\n",
    "    'V16': 0.0,\n",
    "    'V17': 0.0,\n",
    "    'V18': 0.0,\n",
    "    'V19': 0.0,\n",
    "    'V20': 0.0,\n",
    "    'V21': 0.0,\n",
    "    'V22': 0.0,\n",
    "    'V23': 0.0,\n",
    "    'V24': 0.0,\n",
    "    'V25': 0.0,\n",
    "    'V26': 0.0,\n",
    "    'V27': 0.0,\n",
    "    'V28': 0.0,\n",
    "    'Amount': 100.0\n",
    "}\n",
    "\n",
    "# Create input array\n",
    "input_data = np.array([[transaction_data['Time'], transaction_data['V1'], transaction_data['V2'],\n",
    "                        transaction_data['V3'], transaction_data['V4'], transaction_data['V5'],\n",
    "                        transaction_data['V6'], transaction_data['V7'], transaction_data['V8'],\n",
    "                        transaction_data['V9'], transaction_data['V10'], transaction_data['V11'],\n",
    "                        transaction_data['V12'], transaction_data['V13'], transaction_data['V14'],\n",
    "                        transaction_data['V15'], transaction_data['V16'], transaction_data['V17'],\n",
    "                        transaction_data['V18'], transaction_data['V19'], transaction_data['V20'],\n",
    "                        transaction_data['V21'], transaction_data['V22'], transaction_data['V23'],\n",
    "                        transaction_data['V24'], transaction_data['V25'], transaction_data['V26'],\n",
    "                        transaction_data['V27'], transaction_data['V28'], transaction_data['Amount']]])\n",
    "\n",
    "# Scale the input data\n",
    "input_scaled = scaler.transform(input_data)\n",
    "\n",
    "# Make prediction\n",
    "prediction = model.predict(input_scaled)\n",
    "prediction_proba = model.predict_proba(input_scaled)\n",
    "\n",
    "# Display result\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PREDICTION RESULT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if prediction[0] == 1:\n",
    "    print(f\"ðŸš¨ FRAUDULENT TRANSACTION DETECTED! ðŸš¨\")\n",
    "    print(f\"Confidence: {prediction_proba[0][1]*100:.2f}%\")\n",
    "else:\n",
    "    print(f\"âœ… LEGITIMATE TRANSACTION âœ…\")\n",
    "    print(f\"Confidence: {prediction_proba[0][0]*100:.2f}%\")\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f9853e",
   "metadata": {},
   "source": [
    "## ðŸ“Š Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aa063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model comparison table with highlighting\n",
    "comparison_df_styled = comparison_df.style.highlight_max(axis=0)\n",
    "comparison_df_styled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504189bf",
   "metadata": {},
   "source": [
    "## Accuracy vs Precision Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d611d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Accuracy vs Precision chart\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, comparison_df['Accuracy'], width, label='Accuracy', color='#00ff00')\n",
    "bars2 = ax.bar(x + width/2, comparison_df['Precision'], width, label='Precision', color='#0099ff')\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Accuracy vs Precision')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Model'])\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af296866",
   "metadata": {},
   "source": [
    "## Recall vs F1-Score Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960be403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Recall vs F1-Score chart\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, comparison_df['Recall'], width, label='Recall', color='#ff6b6b')\n",
    "bars2 = ax.bar(x + width/2, comparison_df['F1-Score'], width, label='F1-Score', color='#ffd93d')\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Recall vs F1-Score')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Model'])\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9588e0",
   "metadata": {},
   "source": [
    "## Overall Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13452806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create overall performance chart\n",
    "melted_df = comparison_df.melt(id_vars=['Model'], var_name='Metric', value_name='Score')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "models = comparison_df['Model'].unique()\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "x = np.arange(len(models))\n",
    "width = 0.2\n",
    "\n",
    "colors = ['#00ff00', '#0099ff', '#ff6b6b', '#ffd93d']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = comparison_df[metric]\n",
    "    ax.bar(x + i*width, values, width, label=metric, color=colors[i])\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Overall Model Performance Comparison')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7442dae4",
   "metadata": {},
   "source": [
    "## Best Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337ba21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model based on F1-Score\n",
    "best_model_idx = comparison_df['F1-Score'].idxmax()\n",
    "best_model = comparison_df.loc[best_model_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BEST MODEL SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Model: {best_model['Model']}\")\n",
    "print(f\"Accuracy:  {best_model['Accuracy']:.1%}\")\n",
    "print(f\"Precision: {best_model['Precision']:.1%}\")\n",
    "print(f\"Recall:    {best_model['Recall']:.1%}\")\n",
    "print(f\"F1-Score:  {best_model['F1-Score']:.1%}\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
